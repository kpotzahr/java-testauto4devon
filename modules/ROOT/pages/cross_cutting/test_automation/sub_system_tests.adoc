= Subsystem- and System-Tests

This article focuses on Subsystem- and System-Tests. General test automation good practices can be found xref:cross_cutting/test_automation/test_automation.adoc[here].

== General Good Practices

=== Use BDD to develop test scenarios
https://cucumber.io/docs/bdd/[Behavior Driven Development (BDD)] is a software development method that generates a common understanding by conversations with business, developers and testers about how the system should behave.
This prevents problems due to misunderstandings and helps closing gaps in the specification.
The wanted behavior is captured in scenarios with concrete example data in the so called https://cucumber.io/docs/gherkin/reference/[Gherkin] syntax.
The syntax mainly consists of natural language with business terms in a given - when - then - structure.
The scenarios are defined before implementation starts, so that the implementation can take the right direction from the beginning.
BDD tools allow automating the scenarios directly as automated tests.

Typical Tool: https://cucumber.io/[Cucumber]

==== Consider common BDD good and bad practices
Good and bad practices how to write scenarios with Gherkin can be found https://automationpanda.com/2017/01/30/bdd-101-writing-good-gherkin/[here].


Most important:

* *Collaborate* with business, deelopement and test to create scenarios.
Do NOT expect the business or testers to hand them readily over to you as a developer.
* Aim for *short* scenarios (leave out irrelevant data, they can be set in the code as defaults; combine small UI action to user actions; ....).
* Do NOT use generic UI steps, but complete user actions: e.g. instead of "when the user enters "admin" in field "username" the user enters "adminpwd" in field "password" and the user presses the "login" button", use "when the user logs in as administrator"

=== Mix API and UI steps
Just because your expected result can only be checked in the UI does not mean your whole test must be executed via UI.
You can use the API to create you preconditions and the perform some step in the UI.
This makes the complete test faster and more reliable.

=== Prevent waiting for an explicit time
Whenever you have asynchronous behavior in your system under test your test needs to wait until the system has finished the reaction on some asynchronous event.
This is always the case for UI tests when the screen reacts on user interaction, but can also happen in API tests when the system works with asynchronous events.
Avoid waiting an explicit amount of time (dont't use Thread.sleep(5000) or TimeUnit.SECONDS.sleep(5))!
It makes your tests slow and often not even more stable.

Typical tools: 

* http://www.awaitility.org/[awaitility]
* https://www.guru99.com/implicit-explicit-waits-selenium.html[Selenium waiting features] like implicit waiting and WebDriverWait and FluentWait

=== Apply a rerun for failed tests
Even if you do good waiting, asynchronous behavior in the system under test might make your tests flaky.
So, consider to run all failing tests a second time in the pipeline, so that flaky tests do not break your build.
But do not ignore these flaky tests!
Even if you do not break the build you should analyze and stabilize flaky tests.
It helps to keep the test runtimes low and you might otherwise overlook random errors in your application (not only tests can be flaky).

Typical tools: 

* https://maven.apache.org/surefire/maven-surefire-plugin/examples/rerun-failing-tests.html[Maven surefire rerun feature]
* https://medium.com/@belek.bagishbekov/how-to-rerun-failed-test-cases-in-cucumber-b7fe9b1dcf9c[Cucumber rerun feature]


=== Make tests transparent and analyzable by proper reporting
There are three views that need to be adressed by test reporting:

* Business: If you include the business in test creation (which is highly recommended), they will want to see what is tested and that the tests are successful. 
But they will not click through Jenkins builds to see this.

Typical tool: https://www.getxray.app/test-management[Jira Xray Test Management]

* Developers, testers: If a test fails, the reason for the failure must be easiliy analyzable.

Typical tool: https://plugins.jenkins.io/cucumber-reports/[Cucumber Jenkins Plugin]

* Architects, developers, testers: Aggregated views on test runs are needed to detect general improvement needs like runtime optimizations.  

Typical tools:

* https://docs.qameta.io/allure/[Allure reporting] 
* Pushing individual metrics to Kibana or Influx + Grafana to get an aggregated view on execution times and stability (recommended for large-scale projects when the number of tests grows)

== API-related Good Practices

====
*API-test* in this section means a functional test that uses API calls to trigger or check functionality of a fully integrated system under test.
This is a difference to unit testing of a method that exposes an endpoint via REST.
====

Typical Tool: https://rest-assured.io/[RestAssured]

API Test with RestAssured::
+
--
[source, java]
@Test
@DisplayName("Test for getting an existing product by id")
public void testGetProductByIdEndpoint() {
    // arrange
    UUID productId = UUID.fromString("6c4d8ad9-f544-4cc4-8947-113c80fbed07");
    // act
    ProductDto result = when().get("/products/"+productId.toString())
            .then()
            .assertThat()
            .statusCode(200)
            .extract()
            .as(ProductDto.class);
    // assert
    assertThat(result, is(notNullValue()));
    assertThat(result.getId(), is(productId.toString()));
}
--

=== Hide API authentication at a central point

=== Mock external system that are not in your scope
Typical tool: Wiremock


== UI-related Good Practices

====
*UI-test* in this section means a functional test that uses UI calls to trigger or check functionality of a fully integrated system under test.
This is a difference to Unit- or Unit-Integration Tests of a UI components.
====

Typical tool: Selenium

=== Encapsulate UI technology specific logic in page objects

=== Reflect UI components in your test code

=== Add screenshots to the result report


== Test Data Good Practices

=== Keep references to data in the database at a central place

=== Use test data builders with defaults for complex structures

== Test Execution Good Practices

=== Ensure Fail Fast in pipelines
* Parallel exection (surefire or cucumber)
* Tag-wise (e.g. nightly)

=== Simplify environment setup for local tests
A local execution of tests is usually possible without any problems for isolated unit tests.
In contrast, sub-system tests involve databases, message brokers or other external systems that make local execution difficult.

One possibility is to set up the necessary systems using local Docker containers or to connect to corresponding cloud services (e.g. databases).
In both cases, local execution requires manual effort before the tests can be run.
In a microservice architecture with very diverse microservice environments, for example different databases / versions, this can become arbitrarily complex.

link:https://testcontainers.com/[TestContainers] offer a way to set up the necessary environment on-the-fly and without further local configuration.
TestContainers uses Docker containers in the background, which are defined accordingly in the test itself.
For example, a Postgres database and a Kafka message broker can be defined in this way, which are then automatically provided via Docker Containers.
An additional local configuration is no longer necessary.
After the test has been completed, all resources are removed again so that a new test execution is based on a clean environment.
An example usage can be found link:https://testcontainers.com/guides/getting-started-with-testcontainers-for-java/[here].

Quarkus goes one step further with link:https://quarkus.io/guides/dev-services[DevServices].
For example, if a Postgres DB extension is added as a dependency, the corresponding test containers are automatically configured and started in the background when a test is started.
In addition to pure test execution, this also applies to starting the application in dev mode.
However, this is so far limited to the existing DevServices since it is not possible to have user-defined DevServices.